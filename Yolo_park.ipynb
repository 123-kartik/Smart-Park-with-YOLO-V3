{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDWYg0DGvHqN"
   },
   "outputs": [],
   "source": [
    "'''Detecting number of occupied parking spots. Code written keeping in mind the resources provided\n",
    "   under google colab \n",
    "'''\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n",
    "\n",
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iy8ftmQa8amm"
   },
   "outputs": [],
   "source": [
    "!pip install Cython\n",
    "!git clone https://github.com/waleedka/coco\n",
    "!pip install -U setuptools\n",
    "!pip install -U wheel\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LNRzAVNy-izy"
   },
   "outputs": [],
   "source": [
    "#mount your google drive to avoid losing data on colab when session expires \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5AkQbt7l5tP7"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Directory of images to run detection on\n",
    "ROOT_DIR = os.getcwd()\n",
    "# VIDEO_DIR = os.path.join(ROOT_DIR, \"Video\")\n",
    "# VIDEO_SAVE_DIR = os.path.join(VIDEO_DIR, \"save\")\n",
    "# images = list(glob.iglob(os.path.join(VIDEO_SAVE_DIR, '*.*')))\n",
    "# Sort the images by integer index\n",
    "# images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))\n",
    "\n",
    "# outvid = os.path.join(VIDEO_DIR, \"out.mp4\")\n",
    "# make_video(outvid, images, fps=30)\n",
    "\n",
    "# class InferenceConfig(coco.CocoConfig):\n",
    "#     # Set batch size to 1 since we'll be running inference on\n",
    "#     # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "#     GPU_COUNT = 1\n",
    "#     IMAGES_PER_GPU = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hLVvkwHlsiRu"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from util import *\n",
    "from darknet import Darknet\n",
    "from preprocess import prep_image, inp_to_image, letterbox_image\n",
    "import pandas as pd\n",
    "import random \n",
    "import pickle as pkl\n",
    "import argparse\n",
    "\n",
    "\n",
    "def get_test_input(input_dim, CUDA):\n",
    "    img = cv2.imread(\"Khare_frame_02.png\")\n",
    "    img = cv2.resize(img, (input_dim, input_dim)) \n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))\n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0\n",
    "    img_ = torch.from_numpy(img_).float()\n",
    "    img_ = Variable(img_)\n",
    "    \n",
    "    if CUDA:\n",
    "        img_ = img_.cuda()\n",
    "    \n",
    "    return img_\n",
    "\n",
    "def prep_image(img, inp_dim):\n",
    "    \"\"\"\n",
    "    Prepare image for inputting to the neural network. \n",
    "    \n",
    "    Returns a Variable \n",
    "    \"\"\"\n",
    "\n",
    "    orig_im = img\n",
    "    dim = orig_im.shape[1], orig_im.shape[0]\n",
    "    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n",
    "    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n",
    "    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n",
    "    return img_, orig_im, dim\n",
    "\n",
    "def write(x, img, classes):\n",
    "    c1 = tuple(x[1:3].int())\n",
    "    c2 = tuple(x[3:5].int())\n",
    "    cls = int(x[-1])\n",
    "    label = \"{0}\".format(classes[cls])\n",
    "    color = (0,0,255)\n",
    "    cv2.rectangle(img, c1, c2,color, 1)\n",
    "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
    "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
    "    cv2.rectangle(img, c1, c2,color, -1)\n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
    "    return img\n",
    "\n",
    "def arg_parse():\n",
    "    \"\"\"\n",
    "    Parse arguements to the detect module\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='YOLO v3 Video Detection Module')\n",
    "   \n",
    "    parser.add_argument(\"--video\", dest = 'video', help = \n",
    "                        \"Video to run detection upon\",\n",
    "                        default = \"Khare_testvideo_03.mp4\", type = str)\n",
    "    parser.add_argument(\"--dataset\", dest = \"dataset\", help = \"Dataset on which the network has been trained\", default = \"pascal\")\n",
    "    parser.add_argument(\"--confidence\", dest = \"confidence\", help = \"Object Confidence to filter predictions\", default = 0.5)\n",
    "    parser.add_argument(\"--nms_thresh\", dest = \"nms_thresh\", help = \"NMS Threshhold\", default = 0.4)\n",
    "    parser.add_argument(\"--cfg\", dest = 'cfgfile', help = \n",
    "                        \"Config file\",\n",
    "                        default = \"yolov3.cfg\", type = str)\n",
    "    parser.add_argument(\"--weights\", dest = 'weightsfile', help = \n",
    "                        \"weightsfile\",\n",
    "                        default = \"yolov3.weights\", type = str)\n",
    "    parser.add_argument(\"--reso\", dest = 'reso', help = \n",
    "                        \"Input resolution of the network. Increase to increase accuracy. Decrease to increase speed\",\n",
    "                        default = \"1056\", type = str)\n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zWdYhCZzQAB"
   },
   "outputs": [],
   "source": [
    "def yolo_videoprocess():\n",
    "    batch_size = 15\n",
    "    args = arg_parse()\n",
    "    confidence = float(args.confidence)\n",
    "    nms_thesh = float(args.nms_thresh)\n",
    "    start = 0\n",
    "\n",
    "    CUDA = torch.cuda.is_available()\n",
    "\n",
    "    num_classes = 1\n",
    "\n",
    "    CUDA = torch.cuda.is_available()\n",
    "    \n",
    "    bbox_attrs = 5 + num_classes\n",
    "    \n",
    "    print(\"Loading network.....\")\n",
    "    model = Darknet(args.cfgfile)\n",
    "    model.load_weights(args.weightsfile)\n",
    "    print(\"Network successfully loaded\")\n",
    "\n",
    "    model.net_info[\"height\"] = args.reso\n",
    "    inp_dim = int(model.net_info[\"height\"])\n",
    "#     assert inp_dim % 32 == 0 \n",
    "#     assert inp_dim > 32\n",
    "\n",
    "    if CUDA:\n",
    "        model.cuda()\n",
    "        \n",
    "    model(get_test_input(inp_dim, CUDA), CUDA)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    videofile = args.video\n",
    "    \n",
    "    cap = cv2.VideoCapture(videofile)\n",
    "    \n",
    "    assert cap.isOpened(), 'Cannot capture source'\n",
    "    frame_count = 0\n",
    "    frames = []\n",
    "    start = time.time()    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame_count += 1\n",
    "            frames.append(frame)\n",
    "#             print(\"appended\")\n",
    "            if len(frames) == batch_size:\n",
    "              for i, item in enumerate(frames):\n",
    "                frame = item\n",
    "                img, orig_im, dim = prep_image(frame, inp_dim)\n",
    "\n",
    "                im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n",
    "\n",
    "\n",
    "                if CUDA:\n",
    "                    im_dim = im_dim.cuda()\n",
    "                    img = img.cuda()\n",
    "\n",
    "                with torch.no_grad():   \n",
    "                    output = model(Variable(img), CUDA)\n",
    "                output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n",
    "                print(\"count cars {}\".format(output.size(0)))\n",
    "\n",
    "                if type(output) == int:\n",
    "                    frame_count+=1\n",
    "                    print(\"FPS of the video is {:5.2f}\".format( frame_count / (time.time() - start)))\n",
    "                    cv2.imshow(\"frame\", orig_im)\n",
    "                    key = cv2.waitKey(1)\n",
    "                    if key & 0xFF == ord('q'):\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "\n",
    "                im_dim = im_dim.repeat(output.size(0), 1)\n",
    "                scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1)\n",
    "\n",
    "                output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n",
    "                output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n",
    "\n",
    "                output[:,1:5] /= scaling_factor\n",
    "\n",
    "                for i in range(output.shape[0]):\n",
    "                    output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n",
    "                    output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n",
    "\n",
    "                classes = load_classes('coco.names')\n",
    "                colors = pkl.load(open(\"pallete\", \"rb\"))\n",
    "\n",
    "                list(map(lambda x: write(x, orig_im, classes), output))\n",
    "                empty = 60 - output.size(0)\n",
    "                cv2.putText(orig_im, \"Total empty spots: \" + str(empty), (5,30), cv2.FONT_HERSHEY_PLAIN, 1, [0,0,0], 2, cv2.LINE_AA)\n",
    "\n",
    "    #             cv2.imshow(\"Parking 1\", orig_im)\n",
    "    #             frames += 1\n",
    "                name = '{0}.jpg'.format(frame_count + i - batch_size)\n",
    "\n",
    "                name = os.path.join('./ak', name)\n",
    "                cv2.imwrite(name, frame)\n",
    "\n",
    "                \n",
    "                key = cv2.waitKey(1)\n",
    "                if key & 0xFF == ord('q'):\n",
    "                    break\n",
    "              print(\"FPS of the video is {:5.2f}\".format( frame_count / (time.time() - start)))\n",
    "              \n",
    "              frames = []\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    images = list(glob.iglob(os.path.join('./ak', '*.*')))\n",
    "    images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-15]))\n",
    "    \n",
    "    make_video('./outvid/out.mp4', images, fps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "7ToeBIlEdPwY",
    "outputId": "b14b4ffe-acfe-4c4a-d258-f07eedf04560"
   },
   "outputs": [],
   "source": [
    "args = arg_parse()\n",
    "videofile = args.video\n",
    "video = cv2.VideoCapture(videofile)\n",
    "\n",
    "# Find OpenCV version\n",
    "(major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')\n",
    "\n",
    "if int(major_ver)  < 3 :\n",
    "    fps = video.get(cv2.cv.CV_CAP_PROP_FPS)\n",
    "    print(\"Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): {0}\".format(fps))\n",
    "else :\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    print(\"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))\n",
    "\n",
    "video.release();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E-ndlFIy7bXk"
   },
   "outputs": [],
   "source": [
    "# Get all image file paths to a list.\n",
    "# Sort the images by name index.\n",
    "# images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))\n",
    "\n",
    "def make_video(outvid, images=None, fps=15, size=None,\n",
    "               is_color=True, format=\"FMP4\"):\n",
    "    \"\"\"\n",
    "    Create a video from a list of images.\n",
    " \n",
    "    @param      outvid      output video\n",
    "    @param      images      list of images to use in the video\n",
    "    @param      fps         frame per second\n",
    "    @param      size        size of each frame\n",
    "    @param      is_color    color\n",
    "    @param      format      see http://www.fourcc.org/codecs.php\n",
    "    @return                 see http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html\n",
    "    \"\"\"\n",
    "    from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
    "    fourcc = VideoWriter_fourcc(*format)\n",
    "    vid = None\n",
    "    for image in images:\n",
    "        if not os.path.exists(image):\n",
    "            raise FileNotFoundError(image)\n",
    "        img = imread(image)\n",
    "        if vid is None:\n",
    "            if size is None:\n",
    "                size = img.shape[1], img.shape[0]\n",
    "            vid = VideoWriter(outvid, fourcc, float(fps), size, is_color)\n",
    "        if size[0] != img.shape[1] and size[1] != img.shape[0]:\n",
    "            img = resize(img, size)\n",
    "        vid.write(img)\n",
    "    vid.release()\n",
    "    return vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3Z0FbJ_zuOo"
   },
   "outputs": [],
   "source": [
    "yolo_videoprocess() #main program for detecting number of cars in the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsUQlMrCHOWD"
   },
   "outputs": [],
   "source": [
    "!rm -r ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXVPdrk9qsf0"
   },
   "outputs": [],
   "source": [
    "!mkdir ak #don't execute remove and make dir in the same cell on colab, it creates error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "0dwZOANvuT22",
    "outputId": "9aa3e902-d7cd-4d02-da8b-56bf85b02910"
   },
   "outputs": [],
   "source": [
    "#optional code to create video again with different fps\n",
    "images = list(glob.iglob(os.path.join('./ak/', '*.*')))\n",
    "make_video('./outvid/out1.avi', images, fps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T-PFuE8SJkAY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Yolo_park.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
